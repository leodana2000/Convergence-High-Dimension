{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_data_train import One_hidden_layer, generate_data_exp2, train \n",
    "from utils import do_list, do_train\n",
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import nb_neurons, nb_epochs\n",
    "\n",
    "seed = 2222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 : Testing the limit of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first experiment, we wish to understand how many sub-Gaussian examples can be randomly drawn and learned by a 1-layer ReLU network in dimension $d$, and with $p=O(\\log(d))$. Thus, we learn networks with an increasing number of examples in a fixed dimension and look at the probability that the network reaches 0.\n",
    "\n",
    "**Data:** We generated $(x_i,y_i)_{1\\leq i\\leq n}$ *i.i.d* from the following distributions:\n",
    "* $\\frac{x_i}{||x_i||} \\sim \\mathcal{S}^d$ and $||x_i|| \\sim \\mathcal{U}([1,2])$,\n",
    "* $\\frac{y_i}{|y_i|} \\sim \\mathcal{U}(\\{-1,+1\\})$ and $|y_i| \\sim \\mathcal{U}([1,2])$.\n",
    "\n",
    "With these distributions, the data follows the assumption from Lemma 4 in the paper.\n",
    "\n",
    "**Results:** See the analysis notebook and paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXP1.0: Training networks with d=100, n in [1300, 2100], and p varying, with 80 repetition each. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "repetition=80\n",
    "eps=0.05\n",
    "lr=1e0\n",
    "\n",
    "d=100\n",
    "list_n=do_list(min=1300, max=2100, step=50)\n",
    "list_p=[]\n",
    "\n",
    "Loss_trend = []\n",
    "CV_Probability = []\n",
    "for n in list_n:\n",
    "    p = nb_neurons(n, eps)\n",
    "    list_p.append(p)\n",
    "\n",
    "    # Trains network *repetition* times and average the scores.\n",
    "    loss, proba = do_train(d, n, p, lr, repetition)\n",
    "\n",
    "    Loss_trend.append(loss)\n",
    "    CV_Probability.append(proba)\n",
    "\n",
    "results = {\n",
    "        'd': [d]*len(list_n),\n",
    "        'n': list_n,\n",
    "        'p': list_p,\n",
    "        'loss_trend': Loss_trend,\n",
    "        'CV_probability': CV_Probability,\n",
    "    }\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'raw experiments/Data_exp_1_{d}_bis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXP1.1: Measuring the convergence threshold for d=10 to 100, with p=30 fixed and 20 repetitions each. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "eps=0.05\n",
    "list_d=do_list(min=10, max=100, step=10)\n",
    "p=30\n",
    "repetition=20\n",
    "lr=1e0\n",
    "\n",
    "for d in list_d:\n",
    "    list_n = do_list(min=d, max=30*d-200, step=d)\n",
    "\n",
    "    Loss_trend = []\n",
    "    CV_Probability = []\n",
    "    for n in list_n:\n",
    "        \n",
    "        # Trains network *repetition* times and average the scores.\n",
    "        loss, proba = do_train(d, n, p, lr, repetition)\n",
    "\n",
    "        Loss_trend.append(loss)\n",
    "        CV_Probability.append(proba)\n",
    "\n",
    "    results = {\n",
    "        'd': [d]*len(list_n),\n",
    "        'n': list_n,\n",
    "        'p': [p]*len(list_n),\n",
    "        'loss_trend': Loss_trend,\n",
    "        'CV_probability': CV_Probability,\n",
    "    }\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'raw experiments/Data_exp_1_d_{d}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXP1.2: Measuring the convergence threshold for p=350 to 400, with d=30 fixed and 20 repetitions each. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "d=30\n",
    "list_p=do_list(min=350, max=400, step=10)\n",
    "list_n=do_list(min=350, max=450, step=10)\n",
    "repetition=20\n",
    "lr=1e0 \n",
    "\n",
    "for p in list_p:\n",
    "    Loss_trend = []\n",
    "    CV_Probability = []\n",
    "\n",
    "    for n in list_n:\n",
    "        # Trains network *repetition* times and average the scores.\n",
    "        loss, proba = do_train(d, n, p, lr, repetition)\n",
    "\n",
    "        Loss_trend.append(loss)\n",
    "        CV_Probability.append(proba)\n",
    "\n",
    "    results = {\n",
    "        'd': [d]*len(list_n),\n",
    "        'n': list_n,\n",
    "        'p': [p]*len(list_n),\n",
    "        'loss_trend': Loss_trend,\n",
    "        'CV_probability': CV_Probability,\n",
    "    }\n",
    "    data = pd.DataFrame(results)\n",
    "    data.to_csv(f'raw experiments/Data_exp_1_p_{p}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 : Testing the convergence speed conjecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we want to show that conjecture 8 is true: the asymptotical local-PL curvature evolves as $\\frac{1}{\\sqrt{n}}$. We thus train networks with varying number of example ($n$ ranging from 1000 to 2000 orthogonal examples) to compute the rate at which the local-PL curvature $\\mu(t)$ declines. We do so in 3 distinct ways:\n",
    "* We measure the local-PL at the end of the training: $\\mu(t_{\\infty})$ ($t_{\\infty}$ being the last epoch),\n",
    "* We measure the average-PL curvature until the end of the training: $\\langle \\mu_{\\infty} \\rangle$,\n",
    "* We compute lower and upper bounds for the local-PL coefficient (the ones from Lemma 12) and evaluate them at the last epoch: $\\mu_{\\text{low}}$ and $\\mu_{\\text{upp}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXP2: Measuring the convergence speed (local-PL curvature) in dimension d=2000, for n in [1000, 2000], and p varying. We perform 250 repetitions. \"\"\"\n",
    "t.manual_seed(seed)\n",
    "\n",
    "d=2000\n",
    "list_n=do_list(min=1000, max=2000, step=100)\n",
    "repetition=250\n",
    "epsilon=0.05\n",
    "lr=1e0\n",
    "\n",
    "log_Speed = []\n",
    "log_avg_Speed = []\n",
    "log_Lower = []\n",
    "log_Upper = []\n",
    "for n in list_n:\n",
    "    p=nb_neurons(n, epsilon)\n",
    "    epoch=nb_epochs(n, p, lr)*10\n",
    "    \n",
    "    speed = 0.\n",
    "    avg_speed = 0.\n",
    "    lower = 0.\n",
    "    upper = 0.\n",
    "    for rep_id in range(repetition):\n",
    "        print(f\"{n}: {rep_id/repetition}\")\n",
    "        \n",
    "        # To ensure a well initialized network. \n",
    "        flag = True\n",
    "        while flag:\n",
    "            model = One_hidden_layer(emb_dim=d, hid_dim=p)\n",
    "            model.computations[0].weight.data = t.nn.Parameter(model.computations[0].weight.data*0.01)\n",
    "            data = generate_data_exp2(d, n)\n",
    "            if model.is_well_init(data):\n",
    "                Loss = train(model, data, lr, epoch, cv_threshold=1e-6, verbose=False)\n",
    "                flag =  False\n",
    "\n",
    "        window = 100\n",
    "        mu = 2*np.mean((np.array(Loss[-2-window:-2])-np.array(Loss[-1-window:-1]))/(np.array(Loss[-1-window:-1])+np.array(Loss[-2-window:-2])+1e-10)) # change\n",
    "        speed += mu/repetition # Instantaneous speed at the last epoch.\n",
    "\n",
    "        mu_inf = np.log(Loss[0]/Loss[-1])/(len(Loss)*lr)\n",
    "        avg_speed += mu_inf/repetition # Average Speed at the end of the dynamic.\n",
    "\n",
    "        indicator_matrix = (model.computations[0](data[0]) > 0).to(t.float)\n",
    "        square_a_j = (model.computations[2].weight**2).squeeze()\n",
    "        energies = indicator_matrix@square_a_j/p\n",
    "        max_energy = energies.max()*16/(n)\n",
    "        min_energy = energies.min()*2/(n)\n",
    "        lower += min_energy.item()/repetition # The lower bound is measured at the last epoch.\n",
    "        upper += max_energy.item()/repetition # The upper bound is measured at the last epoch.\n",
    "\n",
    "    log_Speed.append(np.log(speed+1e-10))\n",
    "    log_avg_Speed.append(np.log(avg_speed+1e-10))\n",
    "    log_Lower.append(np.log(lower+1e-10))\n",
    "    log_Upper.append(np.log(upper+1e-10))\n",
    "\n",
    "results = {\n",
    "        'n': list_n,\n",
    "        'd': [d]*len(list_n),\n",
    "        'log_speed': log_Speed,\n",
    "        'log_avg_speed': log_avg_Speed,\n",
    "        'log_lower': log_Lower,\n",
    "        'log_upper': log_Upper,\n",
    "    }\n",
    "data = pd.DataFrame(results)\n",
    "data.to_csv(f'raw experiments/Data_exp_2_bis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
